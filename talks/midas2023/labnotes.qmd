---
title: Lab meeting notes
author: Aaron Gerding
format: 
  revealjs:
    width: 1050
    height: 700
revealjs-plugins:
  - attribution
editor: visual
css: styles.css
---

## Losses

Losses are defined on

-   actions x (wait = 0, intervene = 1) and

-   outcomes y (T \> incidence = 0, T \< incidence = 1)

Examples:

$l(x,y) = Cx + L(y-x)_+$

regret form

$C[[x>y]] + (L-C)[[y>x]]$

## Decision rules

Decision rules map current supporting info $z = z_{t_0}$ to future actions $x = x_{t_1}$ where $t_1 = t_0 + h > t_0$

Prior to $t_0$, we can view $Z=Z_{t_0}$ and $Y=Y_{t_1}$ as RV's with joint distribution $P_{ZY}$ - a decision rule $d(w)$ has an expected loss $E_d = E_{ZY}[l(d(Z),Y)]$

Alternatively we can change variables to $X=d(W)$ and define expected loss as $E_d = E_{XY}[l(X,Y)]$

Basic question for a decision maker: which $d$ has minimum $E_d$?

To answer, DM might use an estimate $\widehat{E}_d$ from time series samples from $(Z_t,Y_{t+h})$ or $(X_t,Y_{t+h})$ as stochastic processes

But! What we really care about is $E_d = E_{X_{t_0}, Y_{t_1}}[l(X_{t_0},Y_{t_1})]$ - and $\widehat{E}_d$ might not estimate this with strong stationarity or mixing assumptions

## Decision support info

Types of current info:

Forecast q of class probability - from e.g. logistic regression, or any other model a decision rule can be defined with - a threshold - or as Bayes act for a loss function

Bayes act is the optimal (minimal Eloss) action if forecast is correct

Bayes act for both $l(x,y) = Cx + L(y-x)_+$ and $C[[x>y]] + (L-C)[[y>x]]$ is $[[q > C/L]]$

$E_q[l(x,y)] = q(L-C)[[x=0]] + (1-q)C[[x=1]]$

$x^q = [[(1-q)C < q(L-C)]] = [[\frac{C/L}{1-C/L} < \frac{q}{1-q}]] = [[C/L < q]]$

## Other types of info

Forecast f of right action (deterministic)

-   a decision rule can be defined with

    -   blind faith

    -   sampling to (empirically) impute class probabilities:

        -   wait -\> $P(y=0 \mid f = \text{wait})$,

        -   intervene -\> $P(y=1 \mid f = \text{intervene})$ ,ie., calibration functions

-   also can be done for prob forecasts $rc(q) = P(y=1 \| f = q)$

Forecast of incidence quantile for prob level $1-C/L$

-   decision rule: intervene if quantile \> T

-   forecast is saying $P(y=1) = P(incidence > T) = 1 - P(incidence \leq T) > 1 - 1 + C/L = C/L$

-   assuming reliability

## Scoring rules

Choosing a decision rule for prob forecasts defines a scoring rule $S(q,y) = l(d(q), y)$

-   allows us to define the expected loss of a pforecast as $E(S(Q,Y))$

Bayes scoring rule for $l(x,y) =C[[x>y]] + (L-C)[[y>x]]$ is cost-weighted SR

$$
S_{C/L}(q,y) = y (L-C)[[1-q > 1-C/L]] + (1-y)C [[q \geq C/L]]
$$

Note: We cannot get scoring rules like

-   Brier: $S(q,y) = y(1-q)^2+(1-y) q^2$) or

-   log: $S(q,y) = -y \log (q)-(1-y) \log (1-q)$

this way, but we can write them as mixtures of cost-weighted losses

## Value scores

The 'value score' for a deterministic forecast is

$\frac{E[l(d(m_Y),Y)] - E[l(f,Y)]}{E[l(d(m_Y),Y)] - E[l(Y,Y)]} = \frac{E^l_{clm} -E^l_f}{E^l_{clm} - E^l_{oracle}} = 1-\frac{E^r_f}{E^r_{clm}}$

${clm}$ is the climatology forecast which always

-   the skill score for the loss-based scoring rule applied to q or the imputed probs

These expectations can be computed with respect to the margins ($p_f, p_Y$) and conditionals of $P_{f,Y}$

-   via calibration-refinement (ppv and npv as conditionals); resolution is how much pv depends on f

-   or likelihood-baserate (specificity/fnr, sensitivity/fpr); discrimination is how much likelihood depends on y

```{=tex}
\begin{align}
E_{cl} = 
\end{align}
```
## Deriving the weighted misclassification measure

$$
\frac{E_f}{E_{cl}} = \frac{(L-C)fnp + C fpp}{\max ((L-C)p_Y, C(1-p_Y))} = \frac{(L-C)fnr p_Y + C fpr (1-p_Y)}{\max ((L-C)p_Y, C(1-p_Y))}
$$

$$\frac{E_f}{E_{cl}} = fnr + \frac{C/L}{1-C/L}\frac{1-p_Y}{p_Y}fpr \text{ or } \frac{1-C/L}{C/L}\frac{p_Y}{1-p_Y}fnr + fpr$$
